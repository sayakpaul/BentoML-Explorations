{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Turning machine learning models into APIs in Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to how to create a simple API from a machine learning model in Python using Flask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a the following situation:\n",
    "\n",
    "You have built a super cool machine learning model that can predict if a particular transaction is fraudulent or not. Now, a friend of yours is building an android application for general banking activities and wants to integrate your machine learning model in his application for its super objective. \n",
    "\n",
    "But he found out that, you have coded your model in Python while he is building his application in Java. So? Won't it be possible to integrate your machine learning model into your friend's application?\n",
    "\n",
    "Fortunately enough, you have the power of *APIs*. And the above situation is one of the many one where the need of turning your machine learning models into APIs is extremely important. Many of the industries are now looking for Data Scientists who can do this. Now, wrapping a machine learning model into an API is not very difficult and that is exactly what you will be doing in this tutorial - **Turn your machine learning model into an API**. \n",
    "\n",
    "Specifically, you will be covering the following: \n",
    "- Options to implement machine learning models\n",
    "- What are APIs?\n",
    "- Flask basics\n",
    "- Creating a machine learning model\n",
    "- Saving the machine learning model: Serialization & Deserialization\n",
    "- Creating an API from a machine learning model using Flask\n",
    "- Testing your API in Postman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options to implement Machine Learning models\n",
    "\n",
    "Most of the times, the real use of your machine learning model lies at the heart of an intelligent product – that maybe a small component of a recommender system or an intelligent chat-bot. These are the times when the barriers seem very difficult to overcome.\n",
    "\n",
    "For example, majority of the ML practitioners use R/Python for their experiments. But consumers of those ML models would be software engineers who use a completely different technology stack. There are two ways via which this problem can be solved:\n",
    "\n",
    "- Rewriting the whole code in the language that the software engineering folks work. The above seems like a good idea, but the time & energy required to get those intricate models replicated would be utterly waste. Majority of languages like JavaScript, do not have great libraries to perform ML. One would be wise to stay away from it.\n",
    "- API-first approach – Web APIs have made it easy for cross-language applications to work well. If a frontend developer needs to use your ML Model to create a ML powered web application, he would just need to get the URL Endpoint from where the API is being served.\n",
    "\n",
    "Now, before going any further let's study what really is an API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are APIs?\n",
    "\n",
    "\"In simple words, an API is a (hypothetical) contract between 2 softwares saying if the user software provides input in a pre-defined format, the later with extend its functionality and provide the outcome to the user software.\" - [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-apis-application-programming-interfaces-5-apis-a-data-scientist-must-know/)\n",
    "\n",
    "You can read the following articles to understand why APIs are a popular choice amongst developers:\n",
    "- [History of APIs](http://apievangelist.com/2012/12/20/history-of-apis/)\n",
    "- [Introduction to APIs](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-apis-application-programming-interfaces-5-apis-a-data-scientist-must-know/)\n",
    "\n",
    "Essentially, APIs are very much like web applications but instead of giving you a nicely styled HTML page, APIs tend to return data in a standard data-exchange format such as JSON, XML etc. Once you a developer has the desired output he can style it whatever the way he wants to. There are many popular ML APIs as well for example - *IBM Watson's* ML API which is capable of the following: \n",
    "- Machine Translation - Helps translate text in different language pairs.\n",
    "- Message Resonance – To find out the popularity of a phrase or word with predetermined audience.\n",
    "- Question and Answers - This service provides direct answers to the queries that are triggered by primary document sources.\n",
    "- User Modelling – To make predictions about social characteristics of someone from given text.\n",
    "\n",
    "[Google Vision API](\"https://cloud.google.com/vision/\") is also a very good example which provides dedicated services for Computer Vision tasks. [Click here](\"https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python\") to get an idea of what can be done using Google Vision API.\n",
    "\n",
    "Basically what happens is majority of the cloud providers and smaller machine learning focused companies provide ready-to-use APIs. They cater to the needs of developers / businesses that do not have expertise in ML, who want to implement ML in their processes or product suites.\n",
    "\n",
    "Popular examples of machine learning APIs specifically suited for web development stuffs: [DialogFlow](https://dialogflow.com/), [Microsoft's Cognitive Toolkit](https://www.microsoft.com/en-us/cognitive-toolkit/), [TensorFlow.js](https://js.tensorflow.org/) etc. \n",
    "\n",
    "Now that you have a fair idea of APIs are, let's see how you can wrap a machine learning model (developed in Python) into an API in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask - A web-services' framework in Python:\n",
    "\n",
    "Now, you might think what is a web-service. Web-service is a form of API only that assumes that an API is hosted over a server and can be consumed. Web API, Web Service  - these terms are generally used interchangeably. \n",
    "\n",
    "Coming to Flask, it is a web-service development framework in Python. It is not the only one in Python, there couple others as well such as Django, Falcon, Hug etc. But you will use Flask for this tutorial. For learning about Flask, you can refer [these tutorials](\"https://www.tutorialspoint.com/flask\"). \n",
    "\n",
    "If you downloaded the Anaconda distribution, you already have Flask installed, otherwise, you will have to install it yourself with – \n",
    "```python\n",
    "pip install flask\n",
    "```\n",
    "\n",
    "Flask is very minimal in its kind. Flask is favorite with Python developers for many reasons. Flask framework comes with an inbuilt light-weighted web server which needs absolutely minimal configuration and it can be controlled from your Python code. This is one of the reasons why it is so popular. \n",
    "\n",
    "Following code demonstrate Flask's minimality in a nice way. The code is used to created a simple Web-API which upon receiving a particular URL produces a particular output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Welcome to machine learning model APIs!\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executed, you can navigate to the web address (enter the address on a Web-Browser), which is shown on the terminal, and observe the result.\n",
    "\n",
    "![alt](https://image.ibb.co/jxG4Re/Capture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some points: \n",
    "\n",
    "- **Jupyter Notebooks are great for anything related to markdowns, R and Python. But when it comes to building a web server, it may show inconsistent behavior. So, it is a good idea to write the Flask codes in a text editor like *Sublime* and run the code from terminal/command prompt**. \n",
    "\n",
    "- Make sure you don't name the file as _flask.py_.\n",
    "\n",
    "- Flask runs on port number 5000 by default. Sometimes, the Flask server starts on this port number successfully but when you hit the URL (that the servers returns on the terminal) in a web-browser or any API-client like *Postman*, you may not get the output. Consider the following situation: \n",
    "\n",
    "<img src = \"https://image.ibb.co/iF5hBe/Capture.jpg\"></img>\n",
    "\n",
    "- According to Flask, its server has started successfully on port 5000, but when the URL was fired in the browser, it didn't return anything. So, this can be a possible case of port number conflict. In this case, changing the default port 5000 to your desired port number would be a good choice. You can do that just by doing the following:\n",
    "\n",
    "   ```app.run(debug=True,port=12345)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In that case, the Flask server would look something like the following: \n",
    "\n",
    "<img src = \"https://image.ibb.co/cpkhBe/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go through step by step of the code that you wrote:\n",
    "\n",
    "- You created an instance of the `Flask` class and passed in the \"__name__\" variable (which is filled by Python itself). This variable will be \"__main__\", if this file is being directly run through Python as a script. If you imported the file instead, the value of \"__name__\" will be the name of the file which you imported. For example, if you had `test.py` and `run.py`, and you imported test.py into run.py the \"__name__\" value of test.py will be test (`app = Flask(test)`).\n",
    "\n",
    "- Above `hello()` method definition, there is @app.route(\"/\"). `route()` is a [decorator](https://jeffknupp.com/blog/2013/11/29/improve-your-python-decorators-explained/) that tells Flask what URL should trigger the function defined as `hello()`.\n",
    "\n",
    "- The `hello()` method is responsible producing an output (Welcome to machine learning model APIs!) whenever your API is properly hit (or consumed). In this case, hitting a web-browser with `localhost:5000/` will produce the intended output (provided the flas server is running on port 5000). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now study some of the factors that you will need to keep in mind if you are turning your machine learning models (built using scikit-learn) into a Flask API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn models with Flask\n",
    "\n",
    "Creating very simple to very complex machine learning models have never been this easy in Python with `scikit-learn`. But there are some points you will have to remember about scikit-learn:\n",
    "\n",
    "- [Scikit-learn](http://scikit-learn.org) is a Python library which provides simple and efficient tools for data mining and data analysis. Scikit-learn has the following major modules:\n",
    "    * Clustering\n",
    "    * Regression\n",
    "    * Classification\n",
    "    * Dimensionality Reduction\n",
    "    * Model selection\n",
    "    * Preprocessing\n",
    "\n",
    "<span>(Be sure to check DataCamp's [Supervised Learning with scikit-learn](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn?tap_a=5644-dce66f&tap_s=357540-5b28dd) course which is taught by the core developer of scikit-learn - Andreas Müller)</span>\n",
    "\n",
    "- Scikit-learn provides the support of serialization and de-serialization of the models that you train using scikit-learn. This saves you the time to retrain a model. With a serialized copy of your model made using scikit-learn you can write a Flask API. \n",
    "<br><br>\n",
    "- Scikit-learn models require the data to be in numerical format. That is why, if the dataset contains categorical features that are non-numeric, it is important to convert them into numeric ones. For this transformation scikit-learn provides utilities like `LabelEncoder`, `OneHotEncoder` etc. These can be found in `sklearn.preprocessing` module. \n",
    "<br><br>\n",
    "- Scikit-learn models cannot handle missing values implicitly. You need to handle missing values in your dataset by yourself and then you can feed it to your model. For handling missing values, scikit-learn provides a wide range of utilities which can be found from `sklearn.preprocessing` module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding and missing values are important data preprocessing steps which are very essential for building a good machine learning model. If you want to learn more on this, be sure to check the following course offered by DataCamp:\n",
    "\n",
    "- [Preprocessing for Machine Learning in Python](\"https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, you will use the Titanic dataset which is one of most popular datasets for many reasons such as - the dataset contains a well mixture different types of variables, the dataset contains missing values etc. This [DataCamp tutorial](\"https://www.datacamp.com/community/tutorials/k-means-clustering-python\") covers a good analysis of the dataset and the dataset can be downloaded from [here](\"https://www.kaggle.com/c/titanic/data\").  \n",
    "\n",
    "This dataset deals with a classification problem of predicting if a passenger would survive or not given some information about him/her. \n",
    "\n",
    "**Note**: Variables and Features these terms are used interchangeably at many times in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things even further, you will only use four variables: ```age```, ```sex```, ```embarked```, and ```survived``` where `survived` is the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in a dataframe object and include only four features as mentioned\n",
    "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "df = pd.read_csv(url)\n",
    "include = ['Age', 'Sex', 'Embarked', 'Survived'] # Only four features\n",
    "df_ = df[include]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Sex\" and \"Embarked\" are categorical features with non-numeric values and that is why they require some numeric transformations. “Age” feature has missing values. These values can be imputed with a summary statistic such as median or mean. Missing values can be quite meaningful and it is worth investigating what they represent in real-world applications. \n",
    "\n",
    "Scikit-learn treats the cell values which do not contain anything as NaNs. Here, you will simply to replace NaNs with 0 and you will write a helper function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = []\n",
    "for col, col_type in df_.dtypes.iteritems():\n",
    "     if col_type == 'O':\n",
    "          categoricals.append(col)\n",
    "     else:\n",
    "          df_[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above lines of code does the following:\n",
    "- Iterates over all the columns in the dataframe `df` and appending the columns (with non-numeric values) to a list `categorical`. \n",
    "- If the columns are do not have non-numeric values (which is only `Age` in this case) then it checks if it has missing values or not and fills them with 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Filling NaNs with a single value may have unintended consequences, especially if the value that you’re replacing NaNs with is within the observed range for the numeric variable. Since zero is not an observed and legitimate age value you are not introducing bias, you would have if you used say 36!** - [Source](https://towardsdatascience.com/a-flask-api-for-serving-scikit-learn-models-c8bcdaa41daa)\n",
    "\n",
    "Now that you handled the missing values and separated the non-numeric columns you are ready to convert them to numeric ones. You will do this by using [One Hot Encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder). Pandas provides a simple method ```get_dummies()``` for creating OHE variables for a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ohe = pd.get_dummies(df_, columns=categoricals, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use OHE, new column is created for every column/value combination, in a column_value format. For example -  for the “Embarked” variable, OHE will produce “Embarked_C”, “Embarked_Q”, “Embarked_S”, and “Embarked_nan”.\n",
    "\n",
    "Now that you’ve successfully preprocessed your dataset, you’re ready to train the machine learning model. You will use a _Logistic Regression classifier_ for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "dependent_variable = 'Survived'\n",
    "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
    "y = df_ohe[dependent_variable]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have built your machine learning model. You will now save this model. Technically speaking, you will serialize this model. In Python, you call this **Pickling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model: Serialization and Deserialization\n",
    "\n",
    "You will use sklearn’s ```joblib``` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model is now persisted. You can load this model into memory with a single line of code. Loading the model back into your workspace, is known as Deserialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re now ready to use Flask to serve your persisted model. You have already seen how minimalistic Flask is to get started with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an API from a machine learning model using Flask\n",
    "\n",
    "For serving your model with Flask, you will do the following two things: \n",
    "- Load the already persisted model into memory when the application starts, \n",
    "- Create an API endpoint that takes input variables, transforms them into the appropriate format, and returns predictions.\n",
    "\n",
    "More specifically, your sample input to the API will look like the following:\n",
    "```json\n",
    "[\n",
    "    {\"Age\": 85, \"Sex\": \"male\", \"Embarked\": \"S\"},\n",
    "    {\"Age\": 24, \"Sex\": '\"female\"', \"Embarked\": \"C\"},\n",
    "    {\"Age\": 3, \"Sex\": \"male\", \"Embarked\": \"C\"},\n",
    "    {\"Age\": 21, \"Sex\": \"male\", \"Embarked\": \"S\"}\n",
    "]\n",
    "```\n",
    "(which is a JSON `list` of inputs)\n",
    "\n",
    "and your API will output like the following:\n",
    "```json\n",
    "{\"prediction\": [0, 1, 1, 0]}\n",
    "```\n",
    "\n",
    "The predictions denote the survival statuses where 0 represents No and 1 represents Yes. \n",
    "\n",
    "JSON stands for JavaScript Object Notation and it is one of most widely used data interchange format. If you need a quick introduction to it please follow [these tutorials](https://www.w3schools.com/js/js_json_intro.asp). \n",
    "\n",
    "Let's write a function `predict()` which will do:\n",
    "- Load the persisted model into memory when the application starts, \n",
    "- Create an API endpoint that takes input variables, transforms them into the appropriate format, and returns predictions.\n",
    "\n",
    "You have already seen how to load a persisted model. Now, you will focus on how you can use it for predicting the survival status upon receiving inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "app = Flask(__name__)\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "     json_ = request.json\n",
    "     query_df = pd.DataFrame(json_)\n",
    "     query = pd.get_dummies(query_df)\n",
    "     prediction = lr.predict(query)\n",
    "     return jsonify({'prediction': list(prediction)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! But you have got a little problem here. \n",
    "\n",
    "The function that you wrote would only work under conditions where the incoming request contains all possible values for the categorical variables which may or may not be the case in real-time. If the incoming request does not contain all possible values of the categorical variables then as per the current method definition of `predict()`, `get_dummies()` would generate a dataframe that has less columns than the classifier excepts, which would result in a runtime error. \n",
    "\n",
    "To solve this problem, you will persist the list of columns during model training as well. You can serialize any Python object into a .pkl file. You will use `joblib` the same way as previous.\n",
    "\n",
    "(Keep that in mind, as discussed earlier it is always better to do all the server level coding in a text editor and then run it from a terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_columns.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_columns = list(x.columns)\n",
    "joblib.dump(model_columns, 'model_columns.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have persisted the list of columns already, you can just handle the missing values at the time of prediction. You will have to load model columns when the application starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST']) # Your API endpoint URL would consist /predict\n",
    "def predict():\n",
    "    if lr:\n",
    "        try:\n",
    "            json_ = request.json\n",
    "            query = pd.get_dummies(pd.DataFrame(json_))\n",
    "            query = query.reindex(columns=model_columns, fill_value=0)\n",
    "\n",
    "            prediction = list(lr.predict(query))\n",
    "\n",
    "            return jsonify({'prediction': prediction})\n",
    "\n",
    "        except:\n",
    "\n",
    "            return jsonify({'trace': traceback.format_exc()})\n",
    "    else:\n",
    "        print ('Train the model first')\n",
    "        return ('No model here to use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You included all the required elements in the \"/predict\" API and now, you just need to write the main class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        port = int(sys.argv[1]) # This is for a command-line argument\n",
    "    except:\n",
    "        port = 12345 # If you don't provide any port then the port will be set to 12345\n",
    "    lr = joblib.load(model_file_name) # Load \"model.pkl\"\n",
    "    print ('Model loaded')\n",
    "    model_columns = joblib.load(model_columns_file_name) # Load \"model_columns.pkl\"\n",
    "    print ('Model columns loaded')\n",
    "    app.run(port=port, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your API now ready to be hosted. But before going any further, let's recap all that you did till this point: \n",
    "\n",
    "### Putting it altogether:\n",
    "\n",
    "- You loaded Titanic dataset and selected the four features. \n",
    "- You did the necessary data preprocessing.\n",
    "- You built a Logistic Regression classifier and serialized it. \n",
    "- You also serialized all the columns from training as a solution to the less than expected number of columns is to persist the list of columns from training.\n",
    "- You then wrote a simple API using Flask that would predict if a person was survived in the shipwreck given his age, sex and embarked information. \n",
    "\n",
    "Let's put all the code in one place so that you don't miss out on anything. Also it is a good programming practice if you separate your Logistic Regression model code and your Flask API code into separate `.py` files. \n",
    "\n",
    "So your `model.py` should look like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset in a dataframe object and include only four features as mentioned\n",
    "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "df = pd.read_csv(url)\n",
    "include = ['Age', 'Sex', 'Embarked', 'Survived'] # Only four features\n",
    "df_ = df[include]\n",
    "\n",
    "# Data Preprocessing \n",
    "categoricals = []\n",
    "for col, col_type in df_.dtypes.iteritems():\n",
    "     if col_type == 'O':\n",
    "          categoricals.append(col)\n",
    "     else:\n",
    "          df_[col].fillna(0, inplace=True)\n",
    "\n",
    "df_ohe = pd.get_dummies(df_, columns=categoricals, dummy_na=True)\n",
    "\n",
    "# Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "dependent_variable = 'Survived'\n",
    "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
    "y = df_ohe[dependent_variable]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x, y)\n",
    "\n",
    "# Save your model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'model.pkl')\n",
    "print(\"Model dumped!\")\n",
    "\n",
    "# Load the model that you just saved\n",
    "lr = joblib.load('model.pkl')\n",
    "\n",
    "# Saving the data columns from training\n",
    "model_columns = list(x.columns)\n",
    "joblib.dump(model_columns, 'model_columns.pkl')\n",
    "print(\"Models columns dumped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `api.py` should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.externals import joblib\n",
    "import traceback \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your API definition\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if lr:\n",
    "        try:\n",
    "            json_ = request.json\n",
    "            print(json_)\n",
    "            query = pd.get_dummies(pd.DataFrame(json_))\n",
    "            query = query.reindex(columns=model_columns, fill_value=0)\n",
    "\n",
    "            prediction = list(lr.predict(query))\n",
    "\n",
    "            return jsonify({'prediction': str(prediction)})\n",
    "\n",
    "        except:\n",
    "\n",
    "            return jsonify({'trace': traceback.format_exc()})\n",
    "    else:\n",
    "        print ('Train the model first')\n",
    "        return ('No model here to use')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        port = int(sys.argv[1]) # This is for a command-line input\n",
    "    except:\n",
    "        port = 12345 # If you don't provide any port the port will be set to 12345\n",
    "\n",
    "    lr = joblib.load(\"model.pkl\") # Load \"model.pkl\"\n",
    "    print ('Model loaded')\n",
    "    model_columns = joblib.load(\"model_columns.pkl\") # Load \"model_columns.pkl\"\n",
    "    print ('Model columns loaded')\n",
    "\n",
    "    app.run(port=port, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat! Now you will test this API in an API client called [Postman](\"https://www.getpostman.com/\"). Just make sure that `model.py` and `api.py` are in the same directory and also make sure that you have compiled them both before testing. Refer to the following snapshot of the terminal which is taken after both the `.py` files were compiled successfully.\n",
    "\n",
    "<img src = \"https://image.ibb.co/jgGo2K/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of your files were compiled successfully, then following should be the directory structure:\n",
    "<br>\n",
    "<img src = \"https://image.ibb.co/ee9ShU/Capture.jpg\"></img>\n",
    "\n",
    "**Note**: The IPYNB file is optional though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your API in Postman\n",
    "\n",
    "In order to test your API, you will some kind of API client. Postman is undoubtedly one of the best ones out there. You can easily download Postman from the above link. \n",
    "\n",
    "The Postman interface looks like the following if you downloaded the latest one:\n",
    "<br><img src = \"https://image.ibb.co/hy566e/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have started the Flask server successfully, just enter the right URL with the right port number in Postman. It should look similar to the following: \n",
    "\n",
    "<img src = \"https://image.ibb.co/eCBKbe/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just built your first ever machine learning API. \n",
    "\n",
    "Your API can predict if a passenger survived the titanic shipwreck given his `age`, `sex` and `embarked` information. Now, your friend may call it from his front-end code and process the output of the API into something very exciting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking it further:\n",
    "\n",
    "In this tutorial, you covered one of the most vital industry demanding skills of a full-stack Data Scientist i.e. building an API from a machine learning model. Although the API is very simple, but it is always better to start with simplest of the things so that you know the know-hows in details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot more in order to improve this. Possible options you might want to consider: \n",
    "\n",
    "- Write a \"/train\" API which would train a Logistic Regression classifier with the data. \n",
    "- Code a Neural Network model using `keras` and build an API out of it. \n",
    "- Host your API on Cliud so that it can be consumed.\n",
    "- For taking things to more advanced levels, you might refer to [this Machine Learning Mastery blog](\"https://machinelearningmastery.com/deploy-machine-learning-model-to-production/\") which discusses several industry graded approaches. \n",
    "\n",
    "The possibilities and opportunities are huge here. You just need to carefully select the ones which are the most suitable for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "\n",
    "Following are some references that were taken while writing this blog: \n",
    "\n",
    "- [Flask: Building Python Web Services](\"Flask: Building Python Web Services\")\n",
    "- [A Quora thread on the topic ](\"https://www.quora.com/How-do-I-deploy-Machine-Learning-Models-as-an-API\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
